This repo is all about how machine learning works. From raw data and model training, what are the cleaning steps and checks which should be carried out, for your model to work on that specific domain.

###### Machine Learning is a part of Artificial Intelligence which can learn from provided data to carry out a certain task. The task it learned is particularly for a certain restricted regime, and therefore; and it can not predict anything outside that regime. 


###### Domain specificity is the main limitation of machine learning and deep learning. ML model generally learns a pattern, a function or a mapping in certain regime, and is of no use outside it.

# Data and Data Processing for Machine Learning
We start with the data. Data is an essential part of machine learning. First step is to collect the data for a specific task. You need to collect a sufficiently big and relevant sample for the model to work.
Your sample selection must ensure variance - which means that your data samples should touch each aspect of the regime it is going to be trained in. No variance means no predictability. Once you understand the task you are going to go with, you can decide what sample or features are important for that prediction. There are generally two types of tasks: Supervised Task where you show the model some examples for what it should do given the scenario. And there is another type of task where you just don't have any such ideas. It is called unsupervised learning where all you have is the data and the algorith has to find its way out to big patterns in the data and provide valuable insights.

There are third category of tasks also which is semi-supervised learning where you are able to provide only a small number of cases to model - telling it what it should do. And rest of it is unlabeled as you don't have ability or resources to label it. 

Even in supervised learning, you have two scenarios: you want to group data in certain categories or you want to predict some number or numeric value. First case is called classification and second case is called regression.

In case of classification, you have to decide how many class or categories you have for your use case. Lets say your task is to classify the pictures into dogs and cats. Now you have two categories and you have to collect sufficient data. If you want to classify the birds into respective bird family, say, you have to collect sufficient data for all of your bird families with adequate variability.

Variability is an important aspect in data collection. If you take a birds picture at certain time of the day from certain angle, it will have certain brightness and certain features of bird will be visible clearly and rest might have different brightness, might be obscure or might even be hidden. If all of your pictures have same angle, same brightness, same background and foreground - the model will not be able to generalize. When this model is trained on this identical image data, it will certainly do mistake if new image is shown to model from different angle. It will not be able to recognize it.

Consider other scenario wherein you took pictures from various angles in various geographic locations for the same bird at different times of the day. Now there is a variability in eveverything: brightness, features visible, colors visible, and the aspect ratio of each of its features. If we train a model on this data, it will be trained well. On seeing new totally different image of the same bird, the model can say that it has recognized some of the visual feature from its training while its not certain about rest of them. It will say that it is same bird and it is 70% certain based on resemblance of physical features of the bird, confused for 15% of the feature and totally uncertain for 15% of the features.

Another aspect of data quality or characteristics is that entire training data should have a range of variation. If one takes a picture from 10 KM away and rest of the picture 5 Meter away, at different time of the day, say one in morning and other one in mid night, those images will have differeces - in brightness, in feature aspect ratio relative to entire picture. The picture taken from 10 KM away is an outlier for the rest of the picture. Outlier is a situation where most of other aspects vary in allowed ranges only, and only a few of them do not at all comply with those ranges. In those situations, we process those anamolous scenarios in a way so that we have some usefulness and unity in the data so that it can be used for solving a specific task. This type of treatment is called outlier treatment in statistics.

Also there is some missing features in each of your samples. Some of the pictures got covered by log or tree leaves, and in those pictures, you see missing eyes of that bird. In those scenarios, you replace the numerical value of that eye space with median value of all images you have where eye is visible. This is called missing value imputation wherein missing value in your data is well-thought of and from the value of other features, you calculate and replace the missing or null value.

Last thing to check in your data is how does the data curves look like. This is a test of variability. A uniform curve of any feature means that there is no variability. Bell curve shows that there is high, medium and lows in spread and height. More width means wider ranges in feature values. High peak means well centered and compact data with lesser variability. 

In Machine Learning, you encounter two types of model training scenarios - Magnitude based and Direction based. In magnitude based, you don't care about outlier treatment but still would want to keep the feature distribution more like a bell curve. A bell curve is natural distribution of measures existing in nature such as height and weight of humans. It is also called normal distribution. This type of training is done in clustering, KNN etc. wherein the distance determines the outcome.

In direction based scenarios, you want to check angles between feature values. There, magnitude does not really matter. In those scenarios, having values of different range say one in 10s and another one in 1000s does not make sense. In those cases, you should make sure that each feature is normalized between 0-1, -1 and 1 or -2 to 2 etc. You have various ways to do it - using min max scaling and robust sacling. 

In most of Machine Learning training scenarios, you want each of your feeatures to be normally distributed and follows bell curve. If your data is distributed weirdly and is bent towards the left or right, you first check if the bent is in acceptable range. If it is not acceptable, you transform the feature using various technique such as log transformation, box-cox transformation depending upon the initial insights you get from your data.

Also, one important check one is to do is to check data imbalance if you want to classify your data. If you have minority class(es) with far less amount of sample than other classes beyond say 30:70 or 20:80 in case of two classes, your data is imbalanced. You will have to either downsample if majority class data is important with size irrelavance or upsample the minority class with fake data ceation, fake data which is totally differnt from the minority class but still keeps important aspects of original data intact. SMOTE was extensively used for both upsample and downsampling. But one must be clear about what one wants to achieve from synthetic data of fake data creation and accordingly choose techniques for it.
